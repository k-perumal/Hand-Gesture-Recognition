# ğŸ¤– Hand Gesture Recognition for Sign Language Communication

the project translates American Sign Language (ASL) gestures into text/speech and vice versa using Deep Learning and Computer Vision. This system improves communication for hearing-impaired individuals using a real-time bidirectional interface.

This project is part of our **Final Year Engineering Curriculum** and aims to bridge the communication gap between hearing-impaired and hearing individuals using **Deep Learning**, **Computer Vision**, and **Gesture Recognition**.

---

## ğŸ‘¥ Team Members

- **DHARMA DEVI K** â€“ Final Year Student, B.Tech - Information Technology  
  Arulmigu Meenakshi Amman College of Engineering  
  GitHub: [@DHARMA DEVI K](https://github.com/DHARMADEVIKEERTHI)

- **PERUMAL K** â€“ Final Year Student, B.Tech - Information Technology  
  Arulmigu Meenakshi Amman College of Engineering  
  GitHub: [@PERUMAL K](https://github.com/k-perumal)
  
---

## ğŸ“‚ Dataset Access

Due to file size limits on GitHub, the dataset used for training and testing is hosted on Google Drive.

ğŸ“¥ **[Click here to download the dataset for Hand Sign](https://drive.google.com/drive/folders/1yBXH6Zrcep7F0jpB7UOyxJzoYoiOcU8m)**

ğŸ“¥ **[Click here to download the dataset for Voice Process](https://drive.google.com/drive/folders/1jl_iy0CRS4hsVlHD5J_RLnauXqvP7NwH)**

> âš ï¸ Make sure to download and extract the dataset into the project directory before training or testing the model.

---

## ğŸ“Œ Project Highlights

- ğŸ”¤ **ASL Gesture to Text/Speech**: Recognizes 28 hand gestures using a trained MobileNetV2 model and translates them into readable text and audio.
- ğŸ—£ï¸ **Voice to Sign Animation**: Converts spoken English words to corresponding sign language animations using cartoon-based characters.
- ğŸ¥ **Real-Time Hand Tracking**: Uses MediaPipe for efficient hand landmark detection from webcam feed.
- ğŸ› ï¸ **Command-Line Interface**: A simple menu-driven CLI for smooth interaction and testing.

---

## ğŸ—‚ï¸ Folder Structure
â”œâ”€â”€ asl_model_mobilenetv2.h5 # Trained model for ASL recognition

â”œâ”€â”€ data_collect.py # Script to collect gesture images

â”œâ”€â”€ train.py # Model training script

â”œâ”€â”€ main.py # Main execution: ASL gesture to text/speech

â”œâ”€â”€ voiceto sign.py # Voice input converted to animated sign output

â”œâ”€â”€ requirements.txt # Python dependencies

â”œâ”€â”€ Hand Gesture Recognition Report.pdf # Detailed project documentation

â”œâ”€â”€ PPT.pptx # Presentation slide deck

â””â”€â”€ README.md # Project description (this file)

## 1. Install Dependencies

pip install -r requirements.txt

## 2. Run the Application

### To collect your own gesture data
python data_collect.py

### To train your own model
python train.py

### To recognize gestures and convert to text/speech
python main.py

### To convert voice to sign animation
python "voiceto sign.py"

## ğŸ§  Technologies Used
Python

TensorFlow / Keras

OpenCV

MediaPipe

MobileNetV2

pyttsx3 (Text-to-Speech)

## ğŸ“Š Results
Accuracy: 90%+ on test dataset of 28 ASL signs.

Real-time classification with smooth webcam performance.

Lightweight model deployable on standard devices.

## âœ¨ Future Enhancements
Add more gesture classes for complete ASL vocabulary.

Include multi-hand and dynamic gesture recognition.

Mobile/web deployment with TensorFlow Lite or ONNX.

## ğŸ“œ License
This repository is for academic and educational purposes only. Please contact us for permission before using it in commercial projects.

## ğŸ™Œ Acknowledgment
This project was completed as part of the final semester coursework under the guidance of faculty at Arulmigu Meenakshi Amman College of Engineering.

---

Let me know your Google Drive link and team members if you'd like me to insert them directly into this file. Or if you want a `.md` file to upload, I can generate that too.
